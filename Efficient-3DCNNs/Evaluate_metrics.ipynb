{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4030fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_filename = \"/home/shared/workspace/Resnet3D/3D-ResNets-PyTorch/data/ntu_01.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e12eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_filename = \"/home/shared/workspace/human-activity-recognition/Efficient-3DCNNs/data/results/resnet_101_50_0.001test/val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d07522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a819ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d40b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_groundtruth_pred(ground_truth_filename=None, prediction_filename=None, subset='validation', verbose=False, top_k=1):\n",
    "    if not ground_truth_filename:\n",
    "        raise IOError('Please input a valid ground truth file.')\n",
    "    if not prediction_filename:\n",
    "        raise IOError('Please input a valid prediction file.')\n",
    "\n",
    "    ap = None\n",
    "    hit_at_k = None\n",
    "    # Import ground truth and predictions.\n",
    "    ground_truth, activity_index = _import_ground_truth(ground_truth_filename)\n",
    "    prediction = _import_prediction(prediction_filename, activity_index)\n",
    "\n",
    "    if verbose:\n",
    "        print('[INIT] Loaded annotations from {} subset.'.format(subset))\n",
    "        nr_gt = len(ground_truth)\n",
    "        print('\\tNumber of ground truth instances: {}'.format(nr_gt))\n",
    "        nr_pred = len(prediction)\n",
    "        print('\\tNumber of predictions: {}'.format(nr_pred))\n",
    "        \n",
    "    return ground_truth, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f922de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _import_ground_truth(ground_truth_filename):\n",
    "    \"\"\"Reads ground truth file, checks if it is well formatted, and returns\n",
    "       the ground truth instances and the activity classes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth_filename : str\n",
    "        Full path to the ground truth json file.\n",
    "    Outputs\n",
    "    -------\n",
    "    ground_truth : df\n",
    "        Data frame containing the ground truth instances.\n",
    "    activity_index : dict\n",
    "        Dictionary containing class index.\n",
    "    \"\"\"\n",
    "    with open(ground_truth_filename, 'r') as fobj:\n",
    "        data = json.load(fobj)\n",
    "    # Checking format\n",
    "    # if not all([field in data.keys() for field in self.gt_fields]):\n",
    "        # raise IOError('Please input a valid ground truth file.')\n",
    "\n",
    "    # Initialize data frame\n",
    "    activity_index, cidx = {}, 0\n",
    "    video_lst, label_lst = [], []\n",
    "    for videoid, v in data['database'].items():\n",
    "        if 'validation' != v['subset']:\n",
    "            continue\n",
    "        this_label = v['annotations']['label']\n",
    "        if this_label not in activity_index:\n",
    "            activity_index[this_label] = cidx\n",
    "            cidx += 1\n",
    "        video_lst.append(videoid)\n",
    "        label_lst.append(activity_index[this_label])\n",
    "    ground_truth = pd.DataFrame({'video-id': video_lst,\n",
    "                                 'label': label_lst})\n",
    "    ground_truth = ground_truth.drop_duplicates().reset_index(drop=True)\n",
    "    return ground_truth, activity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06daab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _import_prediction(prediction_filename, activity_index):\n",
    "    \"\"\"Reads prediction file, checks if it is well formatted, and returns\n",
    "       the prediction instances.\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_filename : str\n",
    "        Full path to the prediction json file.\n",
    "    Outputs\n",
    "    -------\n",
    "    prediction : df\n",
    "        Data frame containing the prediction instances.\n",
    "    \"\"\"\n",
    "    with open(prediction_filename, 'r') as fobj:\n",
    "        data = json.load(fobj)\n",
    "    # Checking format...\n",
    "    # if not all([field in data.keys() for field in self.pred_fields]):\n",
    "        # raise IOError('Please input a valid prediction file.')\n",
    "\n",
    "    # Initialize data frame\n",
    "    video_lst, label_lst, score_lst = [], [], []\n",
    "    for videoid, v in data['results'].items():\n",
    "        for result in v:\n",
    "            label = activity_index[result['label']]\n",
    "            video_lst.append(videoid)\n",
    "            label_lst.append(label)\n",
    "            score_lst.append(result['score'])\n",
    "    prediction = pd.DataFrame({'video-id': video_lst,\n",
    "                               'label': label_lst,\n",
    "                               'score': score_lst})\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca8bb9ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Loaded annotations from validation subset.\n",
      "\tNumber of ground truth instances: 1707\n",
      "\tNumber of predictions: 5118\n"
     ]
    }
   ],
   "source": [
    "ground_truth, prediction = load_groundtruth_pred(ground_truth_filename, prediction_filename, 'validation', True, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adfdf598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                       video-id  label\n",
      "0     S014C001P019R001A041_rgb      0\n",
      "1     S016C003P021R001A041_rgb      0\n",
      "2     S008C001P001R001A041_rgb      0\n",
      "3     S013C003P016R001A041_rgb      0\n",
      "4     S009C001P015R002A041_rgb      0\n",
      "...                        ...    ...\n",
      "1702  S016C003P019R002A049_rgb      8\n",
      "1703  S015C003P008R002A049_rgb      8\n",
      "1704  S001C002P006R002A049_rgb      8\n",
      "1705  S013C001P007R002A049_rgb      8\n",
      "1706  S009C003P007R002A049_rgb      8\n",
      "\n",
      "[1707 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a71c5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                       video-id  label     score\n",
      "0     S014C001P019R001A041_rgb      0  0.319168\n",
      "1     S014C001P019R001A041_rgb      5  0.244419\n",
      "2     S014C001P019R001A041_rgb      6  0.175087\n",
      "3     S016C003P021R001A041_rgb      6  0.418693\n",
      "4     S016C003P021R001A041_rgb      3  0.325127\n",
      "...                        ...    ...       ...\n",
      "5113  S001C002P006R002A049_rgb      4  0.204111\n",
      "5114  S001C002P006R002A049_rgb      8  0.164063\n",
      "5115  S013C001P007R002A049_rgb      8  0.782207\n",
      "5116  S013C001P007R002A049_rgb      3  0.084218\n",
      "5117  S013C001P007R002A049_rgb      6  0.048666\n",
      "\n",
      "[5118 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(prediction.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8c2c6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth, prediction, top_k, verbose ):\n",
    "    \"\"\"Evaluates a prediction file. For the detection task we measure the\n",
    "    interpolated mean average precision to measure the performance of a\n",
    "    method.\n",
    "    \"\"\"\n",
    "    hit_at_k, y_pred, y_true, video_name = compute_video_hit_at_k(ground_truth, prediction, top_k)\n",
    "    if verbose:\n",
    "        print('[RESULTS] Performance on ActivityNet untrimmed video '\n",
    "               'classification task.')\n",
    "        print('\\tError@{}: {}'.format(top_k, 1.0 - hit_at_k))\n",
    "        print('\\n')\n",
    "    \n",
    "    if top_k == 1 :\n",
    "        conf_mtx = confusion_matrix(y_true, y_pred)\n",
    "        prec_rec = classification_report(y_true, y_pred)\n",
    "        print('Confusion Matrix\\n')\n",
    "        print(conf_mtx)\n",
    "        print('\\n')\n",
    "        print('Other metrics')\n",
    "        print(prec_rec)\n",
    "    return hit_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ab50201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_video_hit_at_k(ground_truth, prediction, top_k=3):\n",
    "    \"\"\"Compute accuracy at k prediction between ground truth and\n",
    "    predictions data frames. This code is greatly inspired by evaluation\n",
    "    performed in Karpathy et al. CVPR14.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : df\n",
    "        Data frame containing the ground truth instances.\n",
    "        Required fields: ['video-id', 'label']\n",
    "    prediction : df\n",
    "        Data frame containing the prediction instances.\n",
    "        Required fields: ['video-id, 'label', 'score']\n",
    "    Outputs\n",
    "    -------\n",
    "    acc : float\n",
    "        Top k accuracy score.\n",
    "    \"\"\"\n",
    "    video_ids = np.unique(ground_truth['video-id'].values)\n",
    "    avg_hits_per_vid = np.zeros(video_ids.size)\n",
    "    video_name, y_true,y_pred = [],[],[]\n",
    "    for i, vid in enumerate(video_ids):\n",
    "        pred_idx = prediction['video-id'] == vid\n",
    "        if not pred_idx.any():\n",
    "            continue\n",
    "        this_pred = prediction.loc[pred_idx].reset_index(drop=True)\n",
    "        # Get top K predictions sorted by decreasing score.\n",
    "        sort_idx = this_pred['score'].values.argsort()[::-1][:top_k]\n",
    "        this_pred = this_pred.loc[sort_idx].reset_index(drop=True)\n",
    "        # Get labels and compare against ground truth.\n",
    "        pred_label = this_pred['label'].tolist()\n",
    "        gt_idx = ground_truth['video-id'] == vid\n",
    "        gt_label = ground_truth.loc[gt_idx]['label'].tolist()\n",
    "        avg_hits_per_vid[i] = np.mean([1 if this_label in pred_label else 0\n",
    "                                       for this_label in gt_label])\n",
    "        y_pred.append(pred_label)\n",
    "        y_true.append(gt_label)\n",
    "        video_name.append(vid)\n",
    "    return float(avg_hits_per_vid.mean()), y_pred, y_true, video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bd35218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULTS] Performance on ActivityNet untrimmed video classification task.\n",
      "\tError@2: 0.17750439367311077\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8224956063268892"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, prediction, top_k, True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b23c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
